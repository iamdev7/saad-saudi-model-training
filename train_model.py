"""\nTraining Script for Saad - Saudi Arabic Model\nDeveloper: Abdullah Al-Shareef (Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ø§Ù„Ø´Ø±ÙŠÙ)\n\nThis script fine-tunes a language model on Saudi Arabic conversational data\nusing Hugging Face Transformers and PEFT (LoRA) for efficient training.\n"""\n\nimport os\nimport torch\nimport json\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    BitsAndBytesConfig\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    TaskType\n)\n\n@dataclass\nclass ModelConfig:\n    """Configuration for model training"""\n    # Base model to fine-tune\n    base_model: str = "aubmindlab/aragpt2-base"  # or "Qwen/Qwen2-1.5B"\n    \n    # Data paths\n    train_data: str = "data/processed/train.jsonl"\n    val_data: str = "data/processed/validation.jsonl"\n    \n    # Output\n    output_dir: str = "models/saad-saudi-model"\n    \n    # Training hyperparameters\n    num_epochs: int = 3\n    batch_size: int = 4\n    gradient_accumulation_steps: int = 4\n    learning_rate: float = 2e-4\n    max_length: int = 512\n    \n    # LoRA parameters\n    lora_r: int = 16\n    lora_alpha: int = 32\n    lora_dropout: float = 0.05\n    \n    # Quantization\n    use_4bit: bool = True\n    bnb_4bit_compute_dtype: str = "float16"\n    \n\nclass SaadModelTrainer:\n    def __init__(self, config: ModelConfig):\n        self.config = config\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        \n        print(f"ğŸ”§ Device: {self.device}")\n        if self.device == "cuda":\n            print(f"   GPU: {torch.cuda.get_device_name(0)}")\n            print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")\n    \n    def load_model_and_tokenizer(self):\n        """Load base model and tokenizer with quantization"""\n        print(f"\\nğŸ“¥ Loading base model: {self.config.base_model}")\n        \n        # Quantization config for efficient training\n        if self.config.use_4bit and self.device == "cuda":\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_quant_type="nf4",\n                bnb_4bit_compute_dtype=torch.float16,\n                bnb_4bit_use_double_quant=True,\n            )\n        else:\n            bnb_config = None\n        \n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.config.base_model,\n            trust_remote_code=True\n        )\n        \n        # Add special tokens if needed\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        # Load model\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.config.base_model,\n            quantization_config=bnb_config,\n            device_map="auto" if self.device == "cuda" else None,\n            trust_remote_code=True,\n            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32\n        )\n        \n        print("âœ“ Model and tokenizer loaded")\n    \n    def setup_lora(self):\n        """Configure LoRA for parameter-efficient fine-tuning"""\n        print("\\nğŸ§© Setting up LoRA...")\n        \n        # Prepare model for k-bit training\n        if self.config.use_4bit and self.device == "cuda":\n            self.model = prepare_model_for_kbit_training(self.model)\n        \n        # LoRA configuration\n        lora_config = LoraConfig(\n            r=self.config.lora_r,\n            lora_alpha=self.config.lora_alpha,\n            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # Adjust based on model\n            lora_dropout=self.config.lora_dropout,\n            bias="none",\n            task_type=TaskType.CAUSAL_LM\n        )\n        \n        # Get PEFT model\n        self.model = get_peft_model(self.model, lora_config)\n        \n        # Print trainable parameters\n        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n        total_params = sum(p.numel() for p in self.model.parameters())\n        \n        print(f"âœ“ LoRA configured")\n        print(f"   Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")\n        print(f"   Total params: {total_params:,}")\n    \n    def load_datasets(self):\n        """Load and preprocess training datasets"""\n        print("\\nğŸ“ Loading datasets...")\n        \n        # Load JSONL files\n        train_dataset = load_dataset('json', data_files=self.config.train_data, split='train')\n        val_dataset = load_dataset('json', data_files=self.config.val_data, split='train')\n        \n        print(f"âœ“ Datasets loaded")\n        print(f"   Training examples: {len(train_dataset)}")\n        print(f"   Validation examples: {len(val_dataset)}")\n        \n        # Tokenize datasets\n        def tokenize_function(examples):\n            # Convert messages to text format\n            texts = []\n            for messages in examples['messages']:\n                text = ""\n                for msg in messages:\n                    role = msg['role']\n                    content = msg['content']\n                    text += f"<|{role}|>\\n{content}\\n"\n                text += "<|endoftext|>"\n                texts.append(text)\n            \n            # Tokenize\n            return self.tokenizer(\n                texts,\n                truncation=True,\n                max_length=self.config.max_length,\n                padding="max_length"\n            )\n        \n        print("\\nğŸ”„ Tokenizing datasets...")\n        train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n        val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n        \n        self.train_dataset = train_dataset\n        self.val_dataset = val_dataset\n        \n        print("âœ“ Tokenization complete")\n    \n    def train(self):\n        """Train the model"""\n        print("\\nğŸš€ Starting training...")\n        print("="*60)\n        \n        # Training arguments\n        training_args = TrainingArguments(\n            output_dir=self.config.output_dir,\n            num_train_epochs=self.config.num_epochs,\n            per_device_train_batch_size=self.config.batch_size,\n            per_device_eval_batch_size=self.config.batch_size,\n            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n            learning_rate=self.config.learning_rate,\n            weight_decay=0.01,\n            logging_steps=10,\n            save_strategy="epoch",\n            evaluation_strategy="epoch",\n            fp16=True if self.device == "cuda" else False,\n            push_to_hub=False,\n            report_to="none",\n            load_best_model_at_end=True,\n        )\n        \n        # Data collator\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.tokenizer,\n            mlm=False\n        )\n        \n        # Trainer\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=self.train_dataset,\n            eval_dataset=self.val_dataset,\n            data_collator=data_collator,\n        )\n        \n        # Train!\n        trainer.train()\n        \n        print("\\nâœ… Training complete!")\n        \n        # Save final model\n        print(f"\\nğŸ’¾ Saving model to {self.config.output_dir}")\n        trainer.save_model()\n        self.tokenizer.save_pretrained(self.config.output_dir)\n        \n        print("âœ“ Model saved successfully")\n        \n        return trainer\n\n\ndef main():\n    print("="*60)\n    print("    Ù†Ù…ÙˆØ°Ø¬ Ø³Ø¹Ø¯ - ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")\n    print("    Saad Model - Training")\n    print("    Developer: Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ø§Ù„Ø´Ø±ÙŠÙ")\n    print("="*60)\n    print()\n    \n    # Configuration\n    config = ModelConfig()\n    \n    # Check if data files exist\n    if not Path(config.train_data).exists():\n        print("âš ï¸  Training data not found!")\n        print(f"   Please run 'python prepare_data.py' first")\n        return\n    \n    # Initialize trainer\n    trainer_obj = SaadModelTrainer(config)\n    \n    # Training pipeline\n    trainer_obj.load_model_and_tokenizer()\n    trainer_obj.setup_lora()\n    trainer_obj.load_datasets()\n    trainer_obj.train()\n    \n    print("\\n" + "="*60)\n    print("âœ¨ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")\n    print("âœ¨ Training completed successfully!")\n    print("="*60)\n\nif __name__ == '__main__':\n    main()
