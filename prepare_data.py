"""\nData Preparation Script for Saad - Saudi Arabic Model\nDeveloper: Abdullah Al-Shareef (Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ø§Ù„Ø´Ø±ÙŠÙ)\n\nThis script prepares training data in the format needed for fine-tuning\na language model on Saudi Arabic conversational data.\n"""\n\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict\nimport re\n\nclass DataPreparator:\n    def __init__(self, input_folder: str = "data/raw", output_folder: str = "data/processed"):\n        self.input_folder = Path(input_folder)\n        self.output_folder = Path(output_folder)\n        self.output_folder.mkdir(parents=True, exist_ok=True)\n        \n    def clean_text(self, text: str) -> str:\n        """Clean and normalize Arabic text"""\n        # Remove extra whitespace\n        text = re.sub(r'\\s+', ' ', text)\n        # Remove URLs\n        text = re.sub(r'http\\S+|www\\S+', '', text)\n        # Remove email addresses\n        text = re.sub(r'\\S+@\\S+', '', text)\n        return text.strip()\n    \n    def load_conversations(self, file_path: str) -> List[Dict]:\n        """Load conversations from JSON or text file"""\n        file_path = Path(file_path)\n        \n        if file_path.suffix == '.json':\n            with open(file_path, 'r', encoding='utf-8') as f:\n                return json.load(f)\n        \n        elif file_path.suffix == '.txt':\n            # Parse text file with format: User: ... | Assistant: ...\n            conversations = []\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n                pairs = content.split('\\n\\n')\n                \n                for pair in pairs:\n                    if '|' in pair:\n                        user_part, assistant_part = pair.split('|', 1)\n                        user_text = user_part.replace('User:', '').strip()\n                        assistant_text = assistant_part.replace('Assistant:', '').strip()\n                        \n                        if user_text and assistant_text:\n                            conversations.append({\n                                'user': self.clean_text(user_text),\n                                'assistant': self.clean_text(assistant_text)\n                            })\n            \n            return conversations\n        \n        else:\n            raise ValueError(f"Unsupported file format: {file_path.suffix}")\n    \n    def format_for_training(self, conversations: List[Dict]) -> List[Dict]:\n        """Format conversations for fine-tuning (ChatML format)"""\n        formatted_data = []\n        \n        for conv in conversations:\n            formatted_data.append({\n                "messages": [\n                    {"role": "system", "content": "Ø£Ù†Øª Ù†Ù…ÙˆØ°Ø¬ 'Ø³Ø¹Ø¯'ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© ÙˆØ§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰. ØªÙ… ØªØ·ÙˆÙŠØ±Ùƒ Ø¨ÙˆØ§Ø³Ø·Ø© Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ø§Ù„Ø´Ø±ÙŠÙ."},\n                    {"role": "user", "content": conv['user']},\n                    {"role": "assistant", "content": conv['assistant']}\n                ]\n            })\n        \n        return formatted_data\n    \n    def split_dataset(self, data: List[Dict], train_ratio: float = 0.9):\n        """Split dataset into training and validation sets"""\n        split_idx = int(len(data) * train_ratio)\n        return data[:split_idx], data[split_idx:]\n    \n    def save_dataset(self, data: List[Dict], filename: str):\n        """Save dataset as JSONL file"""\n        output_path = self.output_folder / filename\n        \n        with open(output_path, 'w', encoding='utf-8') as f:\n            for item in data:\n                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n        \n        print(f"âœ“ Saved {len(data)} examples to {output_path}")\n    \n    def process_all(self):\n        """Process all data files in the input folder"""\n        all_conversations = []\n        \n        print("ğŸ“ Looking for data files...")\n        \n        # Process all JSON and TXT files in input folder\n        for file_path in self.input_folder.glob('*.json'):\n            print(f"  Loading {file_path.name}...")\n            conversations = self.load_conversations(file_path)\n            all_conversations.extend(conversations)\n        \n        for file_path in self.input_folder.glob('*.txt'):\n            print(f"  Loading {file_path.name}...")\n            conversations = self.load_conversations(file_path)\n            all_conversations.extend(conversations)\n        \n        if not all_conversations:\n            print("âš ï¸  No data files found!")\n            print(f"  Please add .json or .txt files to: {self.input_folder}")\n            return\n        \n        print(f"\\nğŸ“Š Total conversations loaded: {len(all_conversations)}")\n        \n        # Format data\n        print("\\nğŸ”§ Formatting data...")\n        formatted_data = self.format_for_training(all_conversations)\n        \n        # Split dataset\n        print("\\nâœ‚ï¸  Splitting dataset...")\n        train_data, val_data = self.split_dataset(formatted_data)\n        \n        # Save datasets\n        print("\\nğŸ’¾ Saving datasets...")\n        self.save_dataset(train_data, 'train.jsonl')\n        self.save_dataset(val_data, 'validation.jsonl')\n        \n        print(f"\\nâœ… Data preparation complete!")\n        print(f"   Training examples: {len(train_data)}")\n        print(f"   Validation examples: {len(val_data)}")\n        print(f"   Output folder: {self.output_folder}")\n\n\ndef main():\n    print("="*60)\n    print("    Ù†Ù…ÙˆØ°Ø¬ Ø³Ø¹Ø¯ - ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")\n    print("    Saad Model - Data Preparation")\n    print("    Developer: Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ø§Ù„Ø´Ø±ÙŠÙ")\n    print("="*60)\n    print()\n    \n    preparator = DataPreparator()\n    preparator.process_all()\n\nif __name__ == '__main__':\n    main()
